/users/ahjiang/.local/lib/python2.7/site-packages/requests/__init__.py:83: RequestsDependencyWarning: Old version of cryptography ([1, 2, 3]) may cause slowdown.
  warnings.warn(warning, RequestsDependencyWarning)
Setting static random seeds to 1337
==> Building model..
Performing data augmentation on CIFAR10
config sb-start-epoch 1
config lr 0.1
config lr-sched data/config/lr_sched_orig
config momentum 0.9
config decay 0.0005
config batch-size 256
config net mobilenetv2
config dataset cifar10
config seed 1337
config optimizer sgd
config loss-fn cross_reweighted
config sb-strategy sampling
config prob-strategy vanilla
config max-num-backprops 17500000
config sampling-strategy square
config sampling-min 0.1
config sampling-max 1
config prob_pow 1
<function CrossEntropyReweightedLoss at 0x7fbeb46a9b18>
main.py:207: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  softmax_outputs = nn.Softmax()(outputs)
/proj/BigLearning/ahjiang/output/cifar10//190417_batch256_cross_reweighted/pickles/target_confidences/sampling_cifar10_mobilenetv2_1_256_0.0_0.0005_trial1_seed1337_target_confidences.pickle
test_debug,0,0,0,0.023027,10.000000,1555709291.4
/users/ahjiang/src/pytorch-cifar/lib/trainer.py:144: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  softmax_outputs = nn.Softmax()(outputs)
Setting learning rate to 0.1 at 0 backprops
Traceback (most recent call last):
  File "main.py", line 574, in <module>
    main(args)
  File "main.py", line 555, in main
    trainer.train(trainloader)
  File "/users/ahjiang/src/pytorch-cifar/lib/trainer.py", line 124, in train
    self.train_batch(batch, final=False)
  File "/users/ahjiang/src/pytorch-cifar/lib/trainer.py", line 133, in train_batch
    annotated_backward_batch = self.backpropper.backward_pass(backprop_batch)
  File "/users/ahjiang/src/pytorch-cifar/lib/backproppers.py", line 22, in backward_pass
    return self.get_backpropper().backward_pass(*args, **kwargs)
  File "/users/ahjiang/src/pytorch-cifar/lib/backproppers.py", line 66, in backward_pass
    outputs = self.net(data) 
  File "/users/ahjiang/.local/lib/python2.7/site-packages/torch/nn/modules/module.py", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File "/users/ahjiang/.local/lib/python2.7/site-packages/torch/nn/parallel/data_parallel.py", line 121, in forward
    return self.module(*inputs[0], **kwargs[0])
  File "/users/ahjiang/.local/lib/python2.7/site-packages/torch/nn/modules/module.py", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File "/users/ahjiang/src/pytorch-cifar/models/mobilenetv2.py", line 71, in forward
    out = self.layers(out)
  File "/users/ahjiang/.local/lib/python2.7/site-packages/torch/nn/modules/module.py", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File "/users/ahjiang/.local/lib/python2.7/site-packages/torch/nn/modules/container.py", line 91, in forward
    input = module(input)
  File "/users/ahjiang/.local/lib/python2.7/site-packages/torch/nn/modules/module.py", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File "/users/ahjiang/src/pytorch-cifar/models/mobilenetv2.py", line 33, in forward
    out = F.relu(self.bn1(self.conv1(x)))
  File "/users/ahjiang/.local/lib/python2.7/site-packages/torch/nn/functional.py", line 643, in relu
    return torch.relu(input)
RuntimeError: CUDA error: out of memory
